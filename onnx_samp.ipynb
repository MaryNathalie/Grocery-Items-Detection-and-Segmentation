{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA A100-SXM4-40GB\n",
      "PyTorch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Setting up the environment to use only GPU 2\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check number of GPUs\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Check GPU name\n",
    "print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  5 22:18:37 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              64W / 400W |   1346MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              60W / 400W |  10350MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              59W / 400W |   5574MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              85W / 400W |  23876MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              56W / 400W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              62W / 400W |   4000MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              61W / 400W |   2376MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             350W / 400W |  28140MiB / 40960MiB |     23%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   4093229      C   ...eph/anaconda3/envs/myenv/bin/python      614MiB |\n",
      "|    1   N/A  N/A   1802788      C   ...naconda3/envs/mex4_orion/bin/python     5330MiB |\n",
      "|    2   N/A  N/A   4052994      C   ...line/anaconda3/envs/BFCL/bin/python      614MiB |\n",
      "|    3   N/A  N/A   3604423      C   python                                    11824MiB |\n",
      "|    5   N/A  N/A   4058942      C   ...conda3/envs/bertram_mex5/bin/python     3700MiB |\n",
      "|    6   N/A  N/A   3768734      C   ...anaconda3/envs/class_env/bin/python     2078MiB |\n",
      "|    7   N/A  N/A   3782067      C   ...eph/anaconda3/envs/myenv/bin/python    23454MiB |\n",
      "|    7   N/A  N/A   4093229      C   ...eph/anaconda3/envs/myenv/bin/python     2184MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-12-05 22:33:02.493887321 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://eaa12d06959f536d15.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eaa12d06959f536d15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from ultralytics.utils import yaml_load\n",
    "from ultralytics.utils.plotting import Colors\n",
    "\n",
    "# Ensure cache directories exist\n",
    "os.environ[\"GRADIO_TEMP_DIR\"] = \"./gradio_cache\"\n",
    "os.environ[\"GRADIO_CACHE_DIR\"] = \"./gradio_cache\"\n",
    "os.makedirs(\"./gradio_cache\", exist_ok=True)\n",
    "\n",
    "class YOLOv11Seg:\n",
    "    \"\"\"YOLOv11 segmentation model.\"\"\"\n",
    "\n",
    "    def __init__(self, onnx_model):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        Args:\n",
    "            onnx_model (str): Path to the ONNX model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Build Ort session\n",
    "        self.session = ort.InferenceSession(onnx_model,\n",
    "                                            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "                                            if ort.get_device() == 'GPU' else ['CPUExecutionProvider'])\n",
    "\n",
    "        # Numpy dtype: support both FP32 and FP16 onnx model\n",
    "        self.ndtype = np.half if self.session.get_inputs()[0].type == 'tensor(float16)' else np.single\n",
    "\n",
    "        # Get model width and height(YOLOv11-seg only has one input)\n",
    "        self.model_height = 640\n",
    "        self.model_width = 640\n",
    "\n",
    "        # Load COCO class names\n",
    "        self.classes = yaml_load(\"dataset.yaml\")['names']\n",
    "\n",
    "        # Create color palette\n",
    "        self.color_palette = Colors()\n",
    "\n",
    "    def __call__(self, im0, conf_threshold=0.4, iou_threshold=0.45, nm=32):\n",
    "        \"\"\"\n",
    "        The whole pipeline: pre-process -> inference -> post-process.\n",
    "        Args:\n",
    "            im0 (Numpy.ndarray): original input image.\n",
    "            conf_threshold (float): confidence threshold for filtering predictions.\n",
    "            iou_threshold (float): iou threshold for NMS.\n",
    "            nm (int): the number of masks.\n",
    "        Returns:\n",
    "            boxes (List): list of bounding boxes.\n",
    "            segments (List): list of segments.\n",
    "            masks (np.ndarray): [N, H, W], output masks.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pre-process\n",
    "        im, ratio, (pad_w, pad_h) = self.preprocess(im0)\n",
    "\n",
    "        # Ort inference\n",
    "        preds = self.session.run(None, {self.session.get_inputs()[0].name: im})\n",
    "\n",
    "        # Post-process\n",
    "        boxes, segments, masks = self.postprocess(preds,\n",
    "                                                  im0=im0,\n",
    "                                                  ratio=ratio,\n",
    "                                                  pad_w=pad_w,\n",
    "                                                  pad_h=pad_h,\n",
    "                                                  conf_threshold=conf_threshold,\n",
    "                                                  iou_threshold=iou_threshold,\n",
    "                                                  nm=nm)\n",
    "        return boxes, segments, masks\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        \"\"\"\n",
    "        Pre-processes the input image.\n",
    "        Args:\n",
    "            img (Numpy.ndarray): image about to be processed.\n",
    "        Returns:\n",
    "            img_process (Numpy.ndarray): image preprocessed for inference.\n",
    "            ratio (tuple): width, height ratios in letterbox.\n",
    "            pad_w (float): width padding in letterbox.\n",
    "            pad_h (float): height padding in letterbox.\n",
    "        \"\"\"\n",
    "\n",
    "        # Resize and pad input image using letterbox() (Borrowed from Ultralytics)\n",
    "        shape = img.shape[:2]  # original image shape\n",
    "        new_shape = (self.model_height, self.model_width)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        ratio = r, r\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        pad_w, pad_h = (new_shape[1] - new_unpad[0]) / 2, (new_shape[0] - new_unpad[1]) / 2  # wh padding\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(pad_h - 0.1)), int(round(pad_h + 0.1))\n",
    "        left, right = int(round(pad_w - 0.1)), int(round(pad_w + 0.1))\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
    "\n",
    "        # Transforms: HWC to CHW -> BGR to RGB -> div(255) -> contiguous -> add axis(optional)\n",
    "        img = np.ascontiguousarray(np.einsum('HWC->CHW', img)[::-1], dtype=self.ndtype) / 255.0\n",
    "        img_process = img[None] if len(img.shape) == 3 else img\n",
    "        return img_process, ratio, (pad_w, pad_h)\n",
    "\n",
    "    def postprocess(self, preds, im0, ratio, pad_w, pad_h, conf_threshold, iou_threshold, nm=32):\n",
    "        \"\"\"\n",
    "        Post-process the prediction.\n",
    "        Args:\n",
    "            preds (Numpy.ndarray): predictions come from ort.session.run().\n",
    "            im0 (Numpy.ndarray): [h, w, c] original input image.\n",
    "            ratio (tuple): width, height ratios in letterbox.\n",
    "            pad_w (float): width padding in letterbox.\n",
    "            pad_h (float): height padding in letterbox.\n",
    "            conf_threshold (float): conf threshold.\n",
    "            iou_threshold (float): iou threshold.\n",
    "            nm (int): the number of masks.\n",
    "        Returns:\n",
    "            boxes (List): list of bounding boxes.\n",
    "            segments (List): list of segments.\n",
    "            masks (np.ndarray): [N, H, W], output masks.\n",
    "        \"\"\"\n",
    "        x, protos = preds[0], preds[1]  # Two outputs: predictions and protos\n",
    "\n",
    "        # Transpose the first output: (Batch_size, xywh_conf_cls_nm, Num_anchors) -> (Batch_size, Num_anchors, xywh_conf_cls_nm)\n",
    "        x = np.einsum('bcn->bnc', x)\n",
    "\n",
    "        # Predictions filtering by conf-threshold\n",
    "        x = x[np.amax(x[..., 4:-nm], axis=-1) > conf_threshold]\n",
    "\n",
    "        # Create a new matrix which merge these(box, score, cls, nm) into one\n",
    "        # For more details about `numpy.c_()`: https://numpy.org/doc/1.26/reference/generated/numpy.c_.html\n",
    "        x = np.c_[x[..., :4], np.amax(x[..., 4:-nm], axis=-1), np.argmax(x[..., 4:-nm], axis=-1), x[..., -nm:]]\n",
    "\n",
    "        # NMS filtering\n",
    "        x = x[cv2.dnn.NMSBoxes(x[:, :4], x[:, 4], conf_threshold, iou_threshold)]\n",
    "\n",
    "        # Decode and return\n",
    "        if len(x) > 0:\n",
    "\n",
    "            # Bounding boxes format change: cxcywh -> xyxy\n",
    "            x[..., [0, 1]] -= x[..., [2, 3]] / 2\n",
    "            x[..., [2, 3]] += x[..., [0, 1]]\n",
    "\n",
    "            # Rescales bounding boxes from model shape(model_height, model_width) to the shape of original image\n",
    "            x[..., :4] -= [pad_w, pad_h, pad_w, pad_h]\n",
    "            x[..., :4] /= min(ratio)\n",
    "\n",
    "            # Bounding boxes boundary clamp\n",
    "            x[..., [0, 2]] = x[:, [0, 2]].clip(0, im0.shape[1])\n",
    "            x[..., [1, 3]] = x[:, [1, 3]].clip(0, im0.shape[0])\n",
    "\n",
    "            # Process masks\n",
    "            masks = self.process_mask(protos[0], x[:, 6:], x[:, :4], im0.shape)\n",
    "\n",
    "            # Masks -> Segments(contours)\n",
    "            segments = self.masks2segments(masks)\n",
    "            return x[..., :6], segments, masks  # boxes, segments, masks\n",
    "        else:\n",
    "            return [], [], []\n",
    "\n",
    "    @staticmethod\n",
    "    def masks2segments(masks):\n",
    "        \"\"\"\n",
    "        It takes a list of masks(n,h,w) and returns a list of segments(n,xy) (Borrowed from\n",
    "        https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L750)\n",
    "        Args:\n",
    "            masks (numpy.ndarray): the output of the model, which is a tensor of shape (batch_size, 160, 160).\n",
    "        Returns:\n",
    "            segments (List): list of segment masks.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        for x in masks.astype('uint8'):\n",
    "            c = cv2.findContours(x, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0]  # CHAIN_APPROX_SIMPLE\n",
    "            if c:\n",
    "                c = np.array(c[np.array([len(x) for x in c]).argmax()]).reshape(-1, 2)\n",
    "            else:\n",
    "                c = np.zeros((0, 2))  # no segments found\n",
    "            segments.append(c.astype('float32'))\n",
    "        return segments\n",
    "\n",
    "    @staticmethod\n",
    "    def crop_mask(masks, boxes):\n",
    "        \"\"\"\n",
    "        It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box. (Borrowed from\n",
    "        https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L599)\n",
    "        Args:\n",
    "            masks (Numpy.ndarray): [n, h, w] tensor of masks.\n",
    "            boxes (Numpy.ndarray): [n, 4] tensor of bbox coordinates in relative point form.\n",
    "        Returns:\n",
    "            (Numpy.ndarray): The masks are being cropped to the bounding box.\n",
    "        \"\"\"\n",
    "        n, h, w = masks.shape\n",
    "        x1, y1, x2, y2 = np.split(boxes[:, :, None], 4, 1)\n",
    "        r = np.arange(w, dtype=x1.dtype)[None, None, :]\n",
    "        c = np.arange(h, dtype=x1.dtype)[None, :, None]\n",
    "        return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n",
    "\n",
    "    def process_mask(self, protos, masks_in, bboxes, im0_shape):\n",
    "        \"\"\"\n",
    "        Takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher quality\n",
    "        but is slower. (Borrowed from https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L618)\n",
    "        Args:\n",
    "            protos (numpy.ndarray): [mask_dim, mask_h, mask_w].\n",
    "            masks_in (numpy.ndarray): [n, mask_dim], n is number of masks after nms.\n",
    "            bboxes (numpy.ndarray): bboxes re-scaled to original image shape.\n",
    "            im0_shape (tuple): the size of the input image (h,w,c).\n",
    "        Returns:\n",
    "            (numpy.ndarray): The upsampled masks.\n",
    "        \"\"\"\n",
    "        c, mh, mw = protos.shape\n",
    "        masks = np.matmul(masks_in, protos.reshape((c, -1))).reshape((-1, mh, mw)).transpose(1, 2, 0)  # HWN\n",
    "        masks = np.ascontiguousarray(masks)\n",
    "        masks = self.scale_mask(masks, im0_shape)  # re-scale mask from P3 shape to original input image shape\n",
    "        masks = np.einsum('HWN -> NHW', masks)  # HWN -> NHW\n",
    "        masks = self.crop_mask(masks, bboxes)\n",
    "        return np.greater(masks, 0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_mask(masks, im0_shape, ratio_pad=None):\n",
    "        \"\"\"\n",
    "        Takes a mask, and resizes it to the original image size. (Borrowed from\n",
    "        https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L305)\n",
    "        Args:\n",
    "            masks (np.ndarray): resized and padded masks/images, [h, w, num]/[h, w, 3].\n",
    "            im0_shape (tuple): the original image shape.\n",
    "            ratio_pad (tuple): the ratio of the padding to the original image.\n",
    "        Returns:\n",
    "            masks (np.ndarray): The masks that are being returned.\n",
    "        \"\"\"\n",
    "        im1_shape = masks.shape[:2]\n",
    "        if ratio_pad is None:  # calculate from im0_shape\n",
    "            gain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # gain  = old / new\n",
    "            pad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # wh padding\n",
    "        else:\n",
    "            pad = ratio_pad[1]\n",
    "\n",
    "        # Calculate tlbr of mask\n",
    "        top, left = int(round(pad[1] - 0.1)), int(round(pad[0] - 0.1))  # y, x\n",
    "        bottom, right = int(round(im1_shape[0] - pad[1] + 0.1)), int(round(im1_shape[1] - pad[0] + 0.1))\n",
    "        if len(masks.shape) < 2:\n",
    "            raise ValueError(f'\"len of masks shape\" should be 2 or 3, but got {len(masks.shape)}')\n",
    "        masks = masks[top:bottom, left:right]\n",
    "        masks = cv2.resize(masks, (im0_shape[1], im0_shape[0]),\n",
    "                           interpolation=cv2.INTER_LINEAR)  # INTER_CUBIC would be better\n",
    "        if len(masks.shape) == 2:\n",
    "            masks = masks[:, :, None]\n",
    "        return masks\n",
    "\n",
    "    def draw_and_visualize(self, im, bboxes, segments, vis=False, save=True):\n",
    "        \"\"\"\n",
    "        Draw and visualize results.\n",
    "        Args:\n",
    "            im (np.ndarray): original image, shape [h, w, c].\n",
    "            bboxes (numpy.ndarray): [n, 4], n is number of bboxes.\n",
    "            segments (List): list of segment masks.\n",
    "            vis (bool): imshow using OpenCV.\n",
    "            save (bool): save image annotated.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Draw rectangles and polygons\n",
    "        im_canvas = im.copy()\n",
    "        for (*box, conf, cls_), segment in zip(bboxes, segments):\n",
    "\n",
    "            cls_ = int(cls_)\n",
    "            \n",
    "            # draw contour and fill mask\n",
    "            cv2.polylines(im, np.int32([segment]), True, (255, 255, 255), 2)  # white borderline\n",
    "            cv2.fillPoly(im_canvas, np.int32([segment]), self.color_palette(int(cls_), bgr=True))\n",
    "\n",
    "            # draw bbox rectangle\n",
    "            cv2.rectangle(im, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "                          self.color_palette(int(cls_), bgr=True), 1, cv2.LINE_AA)\n",
    "            cv2.putText(im, f'{self.classes[cls_]}: {conf:.3f}', (int(box[0]), int(box[1] - 9)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.color_palette(int(cls_), bgr=True), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Mix image\n",
    "        im = cv2.addWeighted(im_canvas, 0.3, im, 0.7, 0)\n",
    "\n",
    "        # Show image\n",
    "        if vis:\n",
    "            cv2.imshow('demo', im)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        # Save image\n",
    "        if save:\n",
    "            cv2.imwrite('demo.jpg', im)\n",
    "\n",
    "# Initialize the model\n",
    "model = YOLOv11Seg(\"models/run01.onnx\")\n",
    "import time\n",
    "# Define the Gradio interface\n",
    "def inference(image):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Flip the input image horizontally (mirroring for webcam)\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Convert the input image from BGR to RGB for the YOLO model\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes, segments, _ = model(image, 0.3, 0.45)\n",
    "    if len(boxes) > 0:\n",
    "        model.draw_and_visualize(image, boxes, segments, vis=False, save=False)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time) / 10\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    height, width, _ = image.shape\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        f\"FPS: {fps:.2f}\",\n",
    "        (10, height - 10),  # Position FPS text at the bottom-left\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    return image\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=inference,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Input\", sources=\"webcam\", streaming=True),\n",
    "    outputs=gr.Image(type=\"numpy\", label=\"Output\"),\n",
    "    live=True,\n",
    "    title=\"YOLOv11 Segmentation\",\n",
    "    description=\"Upload an image to perform segmentation using YOLOv11.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI231_ME6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
